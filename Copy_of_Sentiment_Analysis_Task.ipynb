{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izu-theintrepid/Calculator/blob/main/Copy_of_Sentiment_Analysis_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42xt85VTJ_ev"
      },
      "source": [
        "# RNNs for Text Classification\n",
        "\n",
        "We will use a RNN based model to perform classification of SMS messages into Spam or not Spam. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwYp_PIQjQxv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBvZraf2jYIG"
      },
      "outputs": [],
      "source": [
        "# Downloading the Spam SMS Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "\n",
        "!unzip /content/smsspamcollection.zip\n",
        "!rm /content/readme\n",
        "!rm !rm /content/smsspamcollection.zip\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enwQxTBKf4S2"
      },
      "outputs": [],
      "source": [
        "# Downloading the GloVe embeddings database\n",
        "\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "!unzip /content/glove.6B.zip\n",
        "\n",
        "!rm -rf /content/glove.6B.zip\n",
        "!rm /content/glove.6B.100d.txt\n",
        "!rm /content/glove.6B.200d.txt\n",
        "!rm /content/glove.6B.300d.txt\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAdzZHGZl3C5"
      },
      "outputs": [],
      "source": [
        "text = []\n",
        "label = []\n",
        "\n",
        "with open(\"/content/SMSSpamCollection\") as f:\n",
        "\n",
        "    \"\"\" read each line of the text file and create a Pandas Data Frame\n",
        "        label spam messages as 1 and legit messages as 0\n",
        "    \"\"\"\n",
        "    for line in f:\n",
        "\n",
        "        label1, message = line.strip().split('\\t', 1)\n",
        "        if label1 == 'spam':\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "\n",
        "        text.append(message)\n",
        "\n",
        "    ###########YOUR CODE HERE###########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRuld-70mCdT"
      },
      "outputs": [],
      "source": [
        "sms = pd.DataFrame(zip(text, label), columns = [\"Text\", \"Label\"])#dataframe\n",
        "text_lengths = []\n",
        "\n",
        "\n",
        "for message in sms[\"Text\"]:\n",
        "    length = 0\n",
        "    for char in message:\n",
        "        length += 1\n",
        "    text_lengths.append(length)#calculating length for every row of data\n",
        "\n",
        "sms['Text_Length'] = text_lengths\n",
        "#sms['Text_Length'] =sms[\"Text\"].apply(len) ###########YOUR CODE HERE###########\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzOJgFYCmTPx"
      },
      "outputs": [],
      "source": [
        "spacy_tokenizer = spacy.load('en_core_web_sm')\n",
        "def tokenize (text):\n",
        "\n",
        "    \"\"\"remove any non-ascii characters\n",
        "       remove punctuations\n",
        "       tokenize the text\n",
        "       return the tokenized text\n",
        "    \"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)#to remove words that contain characters other than alphabets,numbers and _\n",
        "    text = text.strip()\n",
        "    doc = spacy_tokenizer(text)\n",
        "    tokens=[]\n",
        "\n",
        "    for token in doc:\n",
        "      flag = True  #assumption\n",
        "\n",
        "\n",
        "      for char in token.text:\n",
        "        if ord(char) >= 128:  # If non ascii\n",
        "            flag = False\n",
        "            break\n",
        "\n",
        "    # if the token is valid (ascii) and not space, add it to the tokens list\n",
        "\n",
        "      if flag and token.text.strip():\n",
        "        tokens.append(token.text.strip().lower())\n",
        "\n",
        "    return tokens\n",
        "    ###########YOUR CODE HERE###########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5mH9RpmmsEl"
      },
      "outputs": [],
      "source": [
        "tokenized_text = []\n",
        "for text in sms[\"Text\"]:\n",
        "    tokenized_text.append(tokenize(text))    #tokenized version of each text to the list\n",
        "\n",
        "\n",
        "sms[\"Tokenized_Text\"] = tokenized_text\n",
        "\n",
        "\n",
        " ###########YOUR CODE HERE###########"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sms[\"Tokenized_Text\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZmHVaOAYo_h",
        "outputId": "ec369f0c-fe65-45ed-83eb-e09d3024375b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [go, until, jurong, point, crazy, available, o...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
            "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
            "4    [nah, i, do, nt, think, he, goes, to, usf, he,...\n",
            "Name: Tokenized_Text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw0txfuIm29P",
        "outputId": "edefcd7d-3993-4527-baf0-84a5b76cfe55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72909\n"
          ]
        }
      ],
      "source": [
        "def load_GloVe_embeddings(glove_file):\n",
        "    embeddings_dict=dict()\n",
        "    cnt=0\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]  #the word\n",
        "          vector = np.array(values[1:], dtype='float32')  # vector components\n",
        "\n",
        "\n",
        "          embeddings_dict[word] = vector# add the word and its vector to the dictionary\n",
        "          if not word.isalpha():\n",
        "            cnt+=1\n",
        "    \"\"\"\n",
        "        load the GloVe embeddings from the files downloaded\n",
        "        create a dictionary of the form {word : word embedding}\n",
        "    \"\"\"\n",
        "    print(cnt)\n",
        "    return embeddings_dict\n",
        "\n",
        "\n",
        "glove_file = \"/content/glove.6B.50d.txt\"\n",
        "    ###########YOUR CODE HERE###########\n",
        "dict1=load_GloVe_embeddings(glove_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-O4DeKRnjL8"
      },
      "outputs": [],
      "source": [
        "def embed_text(tokenized_text, word_embeddings):\n",
        "    \"\"\"\n",
        "        given a sequence of tokens convert them to their word embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    embedded_text = []\n",
        "\n",
        "\n",
        "    for token in tokenized_text:\n",
        "\n",
        "        embedding_vector = word_embeddings.get(token)\n",
        "        if embedding_vector is not None:#see if the word is in the glove dict\n",
        "            embedded_text.append(embedding_vector)\n",
        "\n",
        "\n",
        "    return np.array(embedded_text,dtype=float)\n",
        "    ###########YOUR CODE HERE###########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmOFLtqbn-2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b82e6883-2c91-4a21-9774-f97f5f00abe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173\n"
          ]
        }
      ],
      "source": [
        "all_word_vector_sequences=[]\n",
        "for token in sms[\"Tokenized_Text\"]:\n",
        "  vec=embed_text(token,dict1)#to see if the word is in dict\n",
        "  if vec.shape[0] == 0:#if the word doesnt exist\n",
        "       vec= np.zeros(shape=(1, 50))\n",
        "\n",
        "  all_word_vector_sequences.append(vec)\n",
        "\n",
        "sms[\"Embedded_Text\"] = all_word_vector_sequences\n",
        "\n",
        "#seeing the max length of a text\n",
        "print(max(sms[\"Embedded_Text\"].apply(len)))###########YOUR CODE HERE###########"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "def padding(X,max_length=174):\n",
        "  tempX=deepcopy(X)\n",
        "  for i,a in enumerate(X):\n",
        "    xlen=a.shape[0]#padding to have uniformity\n",
        "    padlen=max_length-xlen\n",
        "    pad=np.zeros(shape=(padlen,50))\n",
        "    tempX[i]=np.concatenate([a,pad])\n",
        "  return np.array(tempX).astype(float)\n",
        "\n",
        "sms[\"Padded_Embedded_Text\"] = sms[\"Embedded_Text\"].apply(lambda x: padding([x])[0])\n",
        "print(sms[\"Padded_Embedded_Text\"])\n",
        "\n",
        "sms['Label']=sms['Label'].to_numpy().astype(int)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI_P73zObs6U",
        "outputId": "2bcb7d91-01b3-4877-becf-eefdc4dca45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       [[0.14827999472618103, 0.17760999500751495, 0....\n",
            "1       [[-0.5364599823951721, -0.07243199646472931, 0...\n",
            "2       [[-0.41183000802993774, 0.4528000056743622, 0....\n",
            "3       [[-0.25676000118255615, 0.8549000024795532, 1....\n",
            "4       [[0.5095900297164917, 1.2706999778747559, -0.0...\n",
            "                              ...                        \n",
            "5569    [[0.5307400226593018, 0.4011699855327606, -0.4...\n",
            "5570    [[0.8154399991035461, 0.30171000957489014, 0.5...\n",
            "5571    [[-0.05248900130391121, 0.3052400052547455, -0...\n",
            "5572    [[0.4180000126361847, 0.24967999756336212, -0....\n",
            "5573    [[0.7671899795532227, 0.12389999628067017, -0....\n",
            "Name: Padded_Embedded_Text, Length: 5574, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtOWOOv_jYPF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class load_dataset(Dataset):\n",
        "    def __init__(self, padded_texts, labels):\n",
        "        self.padded_texts = padded_texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.padded_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"msg\": torch.tensor(self.padded_texts[idx], dtype=torch.float32),\n",
        "            \"target\": torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6fuH7svjYRr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNN, self).__init__()#calling the parent class\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        #input_size is the size of the input feature or the word embeddings\n",
        "        #hidden_layer is the number of features in the hidden layer\n",
        "        #stacked RNN layers\n",
        "        #batch_first means the input and output tensors are in the shape (batch_size, sequence_length, input_size)\n",
        "        #bidirectional\n",
        "        # Linear layer should map from 512 (concatenated size) to the desired output size (1)\n",
        "        self.fc = nn.Linear(hidden_size * 4, output_size)  # hidden_size * 4 because of bidirectional and pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(2, x.size(0), 128)  # 2 for bidirectional, 128 for hidden size\n",
        "\n",
        "        x, _ = self.rnn(x, h_0)        #RNN forward pass\n",
        "\n",
        "        avg_pool = torch.mean(x, 1)\n",
        "        max_pool, _ = torch.max(x, 1)\n",
        "\n",
        "\n",
        "        out = torch.cat((avg_pool, max_pool), 1)#Concatenate pooled outputs\n",
        "\n",
        "        # fully connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_model(data_loader, model, optimizer, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for data in data_loader:\n",
        "            messages = data[\"msg\"]\n",
        "            targets = data[\"target\"]\n",
        "\n",
        "\n",
        "            messages = messages.to(device, dtype=torch.float32)# Move the data to the device (CPU/GPU)\n",
        "            targets = targets.to(device, dtype=torch.float32)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()#clear the gradients\n",
        "\n",
        "\n",
        "            predictions = model(messages)# make predictions from the model\n",
        "\n",
        "\n",
        "            loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1, 1))#calculate loss\n",
        "\n",
        "\n",
        "            loss.backward()#backpropagate the loss\n",
        "\n",
        "            optimizer.step()   #update the model parameters\n",
        "\n",
        "\n",
        "\n",
        "            epoch_loss += loss.item()# Accumulate the loss for this epoch\n",
        "\n",
        "        #alculate average loss\n",
        "        avg_epoch_loss = epoch_loss / len(data_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e2gFjmxT5RiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(data_loader, model, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            messages=batch[\"msg\"]\n",
        "            targets = batch[\"target\"]  # Unpack the batch\n",
        "\n",
        "            # Debugging\n",
        "            #print(type(reviews), type(targets))  #tensors\n",
        "\n",
        "\n",
        "            messages = messages.to(device) # move the data to the device (CPU)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "\n",
        "            predictions = model(messages)# Make predictions\n",
        "            loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1, 1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            #calculate accuracy\n",
        "            predicted_labels = torch.round(torch.sigmoid(predictions))\n",
        "            correct_predictions += (predicted_labels.cpu() == targets.cpu().view(-1, 1)).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    average_loss = total_loss / len(data_loader)\n",
        "    return average_loss, accuracy"
      ],
      "metadata": {
        "id": "N-Ps-I7i9rz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rap_LDx1jYW4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "X = np.stack(sms[\"Padded_Embedded_Text\"].values)\n",
        "Y = sms[\"Label\"].values\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "# Create the dataset and DataLoader\n",
        "#def Load_dataset(X, Y):\n",
        "    #return load_dataset(torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32))\n",
        "\n",
        "train_dataset = load_dataset(X_train, Y_train)\n",
        "val_dataset = load_dataset(X_val, Y_val)\n",
        "test_dataset = load_dataset(X_test, Y_test)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "input_size = 50  # Since embeddings are of size 50(glove 50)\n",
        "hidden_size = 128\n",
        "num_layers = 1\n",
        "output_size = 1\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize model, optimizer\n",
        "model = RNN(input_size, hidden_size, num_layers, output_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "trained_model = train_model(train_loader, model, optimizer, device, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77aOlM5qSue",
        "outputId": "ce0fe13e-a576-4ff7-ddb5-8feb1cea0a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2579\n",
            "Epoch [2/10], Loss: 0.1386\n",
            "Epoch [3/10], Loss: 0.1131\n",
            "Epoch [4/10], Loss: 0.0934\n",
            "Epoch [5/10], Loss: 0.0756\n",
            "Epoch [6/10], Loss: 0.0690\n",
            "Epoch [7/10], Loss: 0.0645\n",
            "Epoch [8/10], Loss: 0.0511\n",
            "Epoch [9/10], Loss: 0.0418\n",
            "Epoch [10/10], Loss: 0.0335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy = evaluate_model(val_loader, model, device)\n",
        "print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "#evaluate\n",
        "test_loss, test_accuracy = evaluate_model(test_loader, model, device)\n",
        "print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSPQo4W93A45",
        "outputId": "bdab9434-536d-435e-ac45-4b565433a756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.06398799667065894, Validation accuracy: 0.9760765550239234\n",
            "Test loss: 0.07510418437973217, Test accuracy: 0.982078853046595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oJFpVWk5uwh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
